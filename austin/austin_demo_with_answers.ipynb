{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe760763",
   "metadata": {},
   "source": [
    "# Using the Debater services for analysing and finding insights in the Austin Survey dataset \n",
    "In this tutorial we will use a community survey conducted in the city of Austin in the years 2016 and 2017 (https://data.world/cityofaustin/mf9f-kvkk). In this survey, the citizens of Austin where asked \"If there was ONE thing you could share with the Mayor regarding the City of Austin (any comment, suggestion, etc.), what would it be?\". We will analyse their open-ended answers in few different ways.\n",
    "\n",
    "This tutorial will demonstrate how to use the *Argument Quality* service, the *Key Point Analysis (KPA)* service, the *Term Wikifier* service and the *Term Relater* service. It will also demonstrate how they can be combined into a powerful text analysis mechanism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e3fb8",
   "metadata": {},
   "source": [
    "## 1. Run Key Point Analysis on 1000 randomly selected sentences from 2016 survey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac489a",
   "metadata": {},
   "source": [
    "### 1.1 Read random sample of 1000 sentences from 2016 comments\n",
    "We will first read the attached csv file into the 'sentences' parameter. The dataset_austin_sentences.csv file has the Austin survey dataset, after sentences spliting. Each row in the csv is one sentence and a sentence have the following attributes: ['id', 'text', 'district','year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "\n",
    "with open('./dataset_austin_sentences.csv') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    sentences = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770593e5",
   "metadata": {},
   "source": [
    "Lets have a look at the sentences at hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d8262f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d sentences in the dataset' % len(sentences))\n",
    "print('Each sentence is a dictionary with the following keys: %s' % str(sentences[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f971ccb6",
   "metadata": {},
   "source": [
    "Lets select only the sentences from the 2016 survey and randomly sample 1000 out of them. The *Key Point Analysis* service is able to run over hundreds of thousands of sentences, however since the computation is heavy in resources (particularly GPUs) the trial version is limited to 1000 sentences. Using a random.seed(0) is important since we already prepared a hot-cache over these sentences for a quicker *Key Point Analysis* run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe67f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_2016 = [sentence for sentence in sentences if sentence['year'] == '2016']\n",
    "print('There are %d sentences in the 2016 survey' % len(sentences_2016))\n",
    "random.seed(0)\n",
    "random_sample_sentences_2016 = random.sample(sentences_2016, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10ecc6",
   "metadata": {},
   "source": [
    "### 1.2 Run *Key Point Analysis* on the random sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507041f9",
   "metadata": {},
   "source": [
    "Full documentation of the *Key Point Analysis* service can be found [here](https://early-access-program.debater.res.ibm.com/docs/services/keypoints/keypoints_pydoc.html).<br/>\n",
    "Lets initialize few needed parameters. The DebaterApi object supplies the clients for the various debater services. The clients print information using the logger and a suitable verbosity level is needed. The api_key should be set, it can be retrieved from the early-access-program site.  The *Key Point Analysis* service stores the data (and a cache) in a domain. A user can create several domains, one for each dataset. We will run all *Key Point Analysis* jobs in the same domain named 'austin_demo'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd234d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from debater_python_api.api.debater_api import DebaterApi\n",
    "from austin_utils import init_logger\n",
    "import os\n",
    "\n",
    "init_logger()\n",
    "api_key = os.environ['API_KEY']\n",
    "debater_api = DebaterApi(apikey=api_key)\n",
    "keypoints_client = debater_api.get_keypoints_client()\n",
    "domain = 'austin_demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd0c815",
   "metadata": {},
   "source": [
    "Exercise 1:<br/>\n",
    "Lets define a method named *run_kpa*. The method receives a list of sentences (each sentence is a dictionary with the following keys: 'id','text') and runs *Key Point Analysis* on these sentences. In order to run *Key Point Analysis*, we need to:<br/>1. Upload the comments into a domain using the *keypoints_client.upload_comments(domain, comment_ids, comment_texts, dont_split=True)* method. This method receives the domain, a list of comment_ids and a list of comment_texts. When uploading comments into a domain, the *Key Point Analysis* service splits the comments into sentences and runs a minor cleansing on the sentences. Since we already splitted the comments into sentences ourselves and we want to *Key Point Analysis* service to use them as is, we will also set the *dont_split* parameter to True.<br/>2. Wait till all comments in the domain are processed using the *keypoints_client.wait_till_all_comments_are_processed(domain)* method.<br/>3. Start a *Key Point Analysis* job using the *keypoints_client.start_kp_analysis_job(domain, comments_ids, run_params)* method. This method receives the domain, a list of comment_ids and a *run_params*. The run_params is a dictionary with various parameters for cosumizing the job. One of the parameters we can set is *n_top_kps* which tells the system how many key points are required. We will set it to 20, therefore we will use *run_params={'n_top_kps': 20}*. The job runs in an async manner and a future is returned.<br/>4. Use the returned future and wait till results are available using the *future.get_result(high_verbosity=True)* method. The method waits and eventually returns the result. This result is a json with the key points (sorted descendingly according to number of matched sentences) and each key point has a list of matched sentences (sorted descendingly according to their match score). Additional 'none' key point is added. This key point has all the sentences that don't match any key point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f26459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kpa(sentences):\n",
    "    sentences_texts = [sentence['text'] for sentence in sentences]\n",
    "    sentences_ids = [sentence['id'] for sentence in sentences]\n",
    "\n",
    "    keypoints_client.upload_comments(domain=domain,\n",
    "                                     comments_ids=sentences_ids,\n",
    "                                     comments_texts=sentences_texts,\n",
    "                                     dont_split=True)\n",
    "\n",
    "    keypoints_client.wait_till_all_comments_are_processed(domain)\n",
    "\n",
    "    future = keypoints_client.start_kp_analysis_job(domain=domain, comments_ids=sentences_ids,\n",
    "                                                    run_params={'n_top_kps': 20})\n",
    "\n",
    "    kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=5)\n",
    "    return kpa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8aa2cd",
   "metadata": {},
   "source": [
    "We will now use the method you implemented and run over the random sample and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad128a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import print_results\n",
    "\n",
    "kpa_result_random_1000_2016 = run_kpa(random_sample_sentences_2016)\n",
    "print_results(kpa_result_random_1000_2016, n_sentences_per_kp=2, title='Random sample 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4791a0cd",
   "metadata": {},
   "source": [
    "## 2. Run *Key Point Analysis* on 1000 top quality sentences from 2016 survey\n",
    "### 2.1 Select top 1000 sentences from 2016 data using the *Argument Quality* service\n",
    "The Austin Survey dataset is noisy and the answers and sentences vary in quality. Selecting the sentences randomly may lead to running over many sentences that are not informative. Instead, we will now select the more argumentative and informative sentences using the argument-quality service. We will calculate an argument-quality score for each sentence and select 1000 sentences with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb62af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_quality_client = debater_api.get_argument_quality_client()\n",
    "sentences_topic = [{'sentence': sentence['text'], 'topic': 'Austin'} for sentence in sentences_2016]\n",
    "arg_quality_scores = arg_quality_client.run(sentences_topic)\n",
    "sentences_2016_and_scores = zip(sentences_2016, arg_quality_scores)\n",
    "sentences_2016_and_scores_sorted = sorted(sentences_2016_and_scores, key=lambda x: x[1], reverse=True)\n",
    "sentences_2016_sorted = [sentence for sentence, _ in sentences_2016_and_scores_sorted]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af8bf23",
   "metadata": {},
   "source": [
    "Lets examine the top and bottom 10 sentences and check whether the service is able to detect the higher quality sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import split_sentences_to_lines\n",
    "k = 10\n",
    "top_sentences = sentences_2016_sorted[:k]\n",
    "top_sentences = [sentence['text'] for sentence in top_sentences]\n",
    "print('Top %d sentences: ' % k)\n",
    "print('\\n'.join(split_sentences_to_lines(top_sentences, 1)))\n",
    "\n",
    "bottom_sentences = sentences_2016_sorted[-k:]\n",
    "bottom_sentences = [sentence['text'] for sentence in bottom_sentences]\n",
    "print('\\n\\nBottom %d sentences: ' % k)\n",
    "print('\\n'.join(split_sentences_to_lines(bottom_sentences, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a461e98",
   "metadata": {},
   "source": [
    "### 2.2 Run *Key Point Analysis* over the selected sentences\n",
    "We will now run the *run_kpa* method over the top 1000 quality sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab644ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_2016_top_1000_aq = sentences_2016_sorted[:1000]\n",
    "kpa_result_top_aq_1000_2016 = run_kpa(sentences_2016_top_1000_aq)\n",
    "print_results(kpa_result_top_aq_1000_2016, n_sentences_per_kp=2, title='Top aq 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9e7138",
   "metadata": {},
   "source": [
    "### 2.3 Increase coverage by decreasing the matching threshold\n",
    "Exercise 2:<br/>\n",
    "We have reached a nice coverage of ???. In order to increase the coverage a little more, we will add another parameter to the run_param called *mapping_threshold*. We will reimplement the *run_kpa* method (please copy paste the previous one and modify it) but this time the method will also receive a *threshold* parameter and we will use it in the *run_param* in the following way: run_param={'n_top_kps': 20, 'mapping_threshold': threshold}<br/>The mapping_threshold is responsible of deciding whether a sentences matches (supports) a key point. Therefore reducing the threshold from the 0.99 default value makes more sentences match key points and increases the coverage, at the risk of reducing the precision.<br/>In addition, the method will now also return the job_id stored in the future (using the future.get_job_id() method). We will need this job_id in the next excercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d32dec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kpa(sentences, threshold):\n",
    "    sentences_texts = [sentence['text'] for sentence in sentences]\n",
    "    sentences_ids = [sentence['id'] for sentence in sentences]\n",
    "\n",
    "    keypoints_client.upload_comments(domain=domain,\n",
    "                                     comments_ids=sentences_ids,\n",
    "                                     comments_texts=sentences_texts,\n",
    "                                     dont_split=True)\n",
    "\n",
    "    keypoints_client.wait_till_all_comments_are_processed(domain)\n",
    "\n",
    "    future = keypoints_client.start_kp_analysis_job(domain=domain, comments_ids=sentences_ids,\n",
    "                                                    run_params={'n_top_kps': 20, \n",
    "                                                                'mapping_threshold': threshold})\n",
    "\n",
    "    kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=5)\n",
    "    return kpa_result, future.get_job_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba376f7",
   "metadata": {},
   "source": [
    "Lets now run again over the top 1000 quality sentences, this time with a 0.95 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e7854",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_top_aq_1000_2016, kpa_top_aq_1000_2016_job_id = run_kpa(sentences_2016_top_1000_aq, 0.95)\n",
    "print_results(kpa_result_top_aq_1000_2016, n_sentences_per_kp=2, title='Top aq 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222fb888",
   "metadata": {},
   "source": [
    "The coverage was indeed increased to ???. Lets examine the top 5 and bottom 5 sentences that where matched to the first key point and make sure that the precision is still high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba58a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import print_top_and_bottom_matches_for_kp\n",
    "\n",
    "\n",
    "print_top_and_bottom_matches_for_kp(kpa_result_top_aq_1000_2016, 'Traffic congestion needs major improvement', 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0de497",
   "metadata": {},
   "source": [
    "## 3. run over 2017 survey using the key points from 2016 survey\n",
    "### 3.1 Select top 1000 sentences from 2017 data using the *Argument Quality* service\n",
    "It is very useful to be able to compare between different subsets of the data (compare between different years, or different districts). We will now demonstrate how easy it is to  compare the 2017 data to the 2016 data. A similar comparisson can be done between districts or other subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce273c00",
   "metadata": {},
   "source": [
    "Lets first filter the 2017 sentences and take the top 1000 quality sentences, as done for the 2016 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ad109",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_2017 = [sentence for sentence in sentences if sentence['year'] == '2017']\n",
    "sentences_topic = [{'sentence': sentence['text'], 'topic': 'Austin'} for sentence in sentences_2017]\n",
    "arg_quality_scores = arg_quality_client.run(sentences_topic)\n",
    "sentences_2017_and_scores = zip(sentences_2017, arg_quality_scores)\n",
    "sentences_2017_and_scores_sorted = sorted(sentences_2017_and_scores, key=lambda x: x[1], reverse=True)\n",
    "sentences_2017_sorted = [sentence for sentence, _ in sentences_2017_and_scores_sorted]\n",
    "sentences_2017_top_1000_aq = sentences_2017_sorted[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829fb43c",
   "metadata": {},
   "source": [
    "### 3.2 Run *Key Point Analysis* over the selected sentence using the key points from 2016\n",
    "Exercise 3:<br/>\n",
    "In order to compare the 2017 sentences to 2016 sentences we will want to map the 2017 sentences to the same key points extracted on the 2016 sentences (therwise different key points could be automattically extracted on the 2017 sentences and it would be hard to compare between them).\n",
    "For this end we will reimplement the *run_kpa* method (please copy paste the previous one and modify it). This time it will receive a new *key_points_by_job_id* parameter. This parameter is passed to the *key_points_by_job_id* parameter in the *keypoints_client.start_kp_analysis_job(domain, comments_ids,run_params, key_points_by_job_id)* method. When *None* is passed to *key_points_by_job_id*, key points are automatically extracted. However when it is set with a *job_id* of a previous job it uses the key points from that job's result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db92e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kpa(sentences, threshold, key_points_by_job_id=None):\n",
    "    sentences_texts = [sentence['text'] for sentence in sentences]\n",
    "    sentences_ids = [sentence['id'] for sentence in sentences]\n",
    "\n",
    "    keypoints_client.upload_comments(domain=domain,\n",
    "                                     comments_ids=sentences_ids,\n",
    "                                     comments_texts=sentences_texts,\n",
    "                                     dont_split=True)\n",
    "\n",
    "    keypoints_client.wait_till_all_comments_are_processed(domain)\n",
    "\n",
    "    future = keypoints_client.start_kp_analysis_job(domain=domain, comments_ids=sentences_ids,\n",
    "                                                    run_params={'n_top_kps': 20, \n",
    "                                                                'mapping_threshold': threshold},\n",
    "                                                    key_points_by_job_id=key_points_by_job_id)\n",
    "\n",
    "    kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=5)\n",
    "    return kpa_result, future.get_job_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9060636",
   "metadata": {},
   "source": [
    "Lets use the new *run_kpa* and provide it with the *top 1000 quality sentences from 2017* and the job_id of *top 1000 quality sentences from 2016*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba5692",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_top_aq_1000_2017, _ = run_kpa(sentences_2017_top_1000_aq, 0.95, kpa_top_aq_1000_2016_job_id)\n",
    "print_results(kpa_result_top_aq_1000_2017, n_sentences_per_kp=2, title='Top aq 2017, using 2016 key points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bd34a7",
   "metadata": {},
   "source": [
    "Since both jobs have the same key points, we can now easily compare the two results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11c93f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from austin_utils import compare_results\n",
    "\n",
    "compare_results(kpa_result_top_aq_1000_2016, '2016', kpa_result_top_aq_1000_2017, '2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99414637",
   "metadata": {},
   "source": [
    "## 4. Expend the traffic-problem in Austin using the *Term Wikifier* and *Term Relater* services\n",
    "As we've seen in the 2016 results, that the traffic problem in Austin is significant. In this section we will use the *Term Wikifier* and *Term Relater* service to select a subset of the sentences that mention and are related to the traffic topic and run *Key Point Analysis* over them. This will help us create key points specifically to the traffic problem and expose relevant insights and suggestions on the topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae6ce75",
   "metadata": {},
   "source": [
    "### 4.1 Calculate the mentions in the sentences using the *Term Wikifier*\n",
    "Exercise 4:<br/>\n",
    "Lets use the *Term Wikifier* service and calculate the mentions for each sentence and store it in a dictionary named *sentence_to_mentions*. The method receives sentences_texts (a list of the sentences texts as strings) and runs the *Term Wikifier* over them. The *Term Wikifier* runs ove all sentences_texts and returns a list of mentions_lists. One mentions_list for each sentence_text. Each mention is a dictionary. We will extract the mention title this way: mention['concept']['title']."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcde913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_to_mentions(sentences_texts):\n",
    "    term_wikifier_client = debater_api.get_term_wikifier_client()\n",
    "    mentions_list_list = term_wikifier_client.run(sentences_texts)\n",
    "    sentence_to_mentions = {}\n",
    "    for sentence_text, mentions_list in zip(sentences_texts, mentions_list_list):\n",
    "        sentence_to_mentions[sentence_text] = set([mention['concept']['title'] for mention in mentions_list])\n",
    "    return sentence_to_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe97f270",
   "metadata": {},
   "source": [
    "Lets get the text of 2016 sentences and get their mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7319ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_2016_texts = [sentence['text'] for sentence in sentences_2016]\n",
    "sentence_to_mentions = get_sentence_to_mentions(sentences_2016_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416edae2",
   "metadata": {},
   "source": [
    "### 4.2 Find the mentions that relate to the *traffic* concept using the *Term Relater* service\n",
    "Since we're inrested in the *traffic* concept, we will now take all mentions and find the ones that are related to that concept. Then we will select all sentences that have at least one mention that is related to the *traffic* concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca2cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mentions = [mention for sentence in sentence_to_mentions \n",
    "                   for mention in sentence_to_mentions[sentence]]\n",
    "all_mentions = sorted(list(set(all_mentions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992e47f9",
   "metadata": {},
   "source": [
    "Exercise 5:<br/>\n",
    "Implement the *get_related_mentions(concept, threshold, all_mentions)* method. It receives a given concept, a threshold and all_mentions. It then uses the *Term Relater* service to calculate the relatedness between the mentions and the concept and returns all mentions that have relatedness score above the given threhold. The *term_relater_client* receives a list of pairs (each pair is a list with two mentions) and returns a list of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b85b9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_mentions(concept, threshold, all_mentions):\n",
    "    term_relater_client = debater_api.get_term_relater_client()\n",
    "    concept_mention_pairs = [[concept, mention] for mention in all_mentions]\n",
    "    scores = term_relater_client.run(concept_mention_pairs)\n",
    "    return [mention for mention, score in zip(all_mentions, scores) if score > threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae3c340",
   "metadata": {},
   "source": [
    "We will now use the method you've implemented and find the mentions that match the *traffic* concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471cea81",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_mentions = get_related_mentions('traffic', 0.5, all_mentions)\n",
    "print(matched_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a4f140",
   "metadata": {},
   "source": [
    "### 4.3 Run *Key Point Analysis* over the sentences that relate to the *traffic* concept\n",
    "Let's select the sentences that have mentions that are related to the *traffic* concept and run over them. We will need to switch back from sentences_texts to sentences dictionaries since our *run_kpa* method needs the sentnces dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97183db",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences_texts = [sentence for sentence in sentences_2016_texts \n",
    "                     if len(sentence_to_mentions[sentence].intersection(matched_mentions)) > 0]\n",
    "print('Running over %d sentences' % len(matched_sentences_texts))\n",
    "matched_sentences = [sentence for sentence in sentences_2016 if sentence['text'] in matched_sentences_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7713d3",
   "metadata": {},
   "source": [
    "Finally, lets run over these sentences and examine the *traffic* related key points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a119e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_traffic_2016, _ = run_kpa(matched_sentences, None)\n",
    "print_results(kpa_result_traffic_2016, n_sentences_per_kp=2, title='Traffic KPA 2016')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
