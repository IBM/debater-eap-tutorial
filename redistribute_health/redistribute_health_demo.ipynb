{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Run pro con service on arguments collected from the crowd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with reading the arguments from a csv file, define the topic, and create a debater_api object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from debater_python_api.api.debater_api import DebaterApi\n",
    "from debater_python_api.api.clients.narrative_generation_client import Polarity\n",
    "import csv\n",
    "\n",
    "\n",
    "api_key = os.environ['API_KEY']\n",
    "debater_api = DebaterApi(api_key)\n",
    "arguments_file = 'wealth_test_set.csv'\n",
    "topic = 'It is time to redistribute the wealth'\n",
    "\n",
    "with open(arguments_file, encoding='utf8') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    examples = list(reader)\n",
    "    arguments = [example['sentence'] for example in examples]\n",
    "    labels = [example['label'] for example in examples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new we are ready to run the pro con service. The pro con service gets a list of dictionaries where each dictionary contains two keys: an argument and a topic. (e.g. arg_topic_dicts = [{'sentence': 'first sentence', 'topic': 'first topic'}, {'sentence': 'second sentence', 'topic': 'second topic'}] \n",
    "\n",
    "The service returns a list of floats in the range [-1, 1], where each float is the prediction (score) for the corresponding sentence and topic. The sign of the score indicated if the service predicts that the argument supports the motion (positive score), or contests the topic (negative score). The absolute value of the score indicated the confidence of the service. A low value indicates low confidence, while a high value indicates strong confidenec.\n",
    "\n",
    "Please write a code that create a pro_con_client by calling the method get_pro_con_client() of the debater_api object.\n",
    "Create a list with the arguments we got from the crowd while using the topic we defined, and run the pro con client on the list, while storing the results at a list with the name pro_con_scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pro_con_client = debater_api.get_pro_con_client()\n",
    "sentence_topic_dicts = [{'sentence': argument, 'topic': topic} for argument in arguments]\n",
    "pro_con_scores = pro_con_client.run(sentence_topic_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Use pro con results\n",
    "Now, we can use the service results and measure it accuracy. We will compare the service prediction to the labels created by human annotators. The label field contains tree values: '1' for supporting arguments, '-1' for contesting arguments, and '0' for nuetral or mixed arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "argument_score_label_list = list(zip(arguments, pro_con_scores, labels))\n",
    "\n",
    "predicted_pro = [triple for triple in argument_score_label_list if triple[1] > 0]\n",
    "predicted_pro_labeled_pro = [triple for triple in predicted_pro if triple[2] == '1']\n",
    "print('predicted pro accuracy {}'.format(len(predicted_pro_labeled_pro) / len(predicted_pro)))\n",
    "\n",
    "predicted_con = [triple for triple in argument_score_label_list if triple[1] < 0]\n",
    "predicted_con_labeled_con = [triple for triple in predicted_con if triple[2] == '-1']\n",
    "print('predicted con accuracy {}'.format(len(predicted_con_labeled_con) / len(predicted_con)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate coverage - precision curve\n",
    "\n",
    "as we saw in the previous run, although most of the time the prediction is right, still we have about 30% errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wealth_util import calculate_statistics, plot_graph\n",
    "\n",
    "pro_con_statistics = calculate_statistics('./wealth_test_set.csv', pro_con_scores)\n",
    "plot_graph(examples_list=[pro_con_statistics], labels=['prod'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Train pro - con service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Generate narrative using the fine tuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
