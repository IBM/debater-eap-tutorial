{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Austin tutorial.\n",
    "In this tutorial we will use a community survey from Austin Texas...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. KPA on 2016 random 1000 senetences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.A Read random sample of 1000 sentences from 2016 comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "\n",
    "with open('./dataset_austin_sentences.csv') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    sentences = list(reader)\n",
    "        \n",
    "sentences_2016 = list(filter(lambda sentence: sentence['Year'] == '2016', sentences))\n",
    "random.seed(0)\n",
    "random_sample_sentences_2016 = random.sample(sentences_2016, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.B Run KPA on the random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from debater_python_api.api.debater_api import DebaterApi\n",
    "from debater_python_api.api.clients.keypoints_client import KpAnalysisUtils \n",
    "\n",
    "\n",
    "KpAnalysisUtils.init_logger()\n",
    "api_key = '<api-key>'\n",
    "debater_api = DebaterApi(apikey=api_key)\n",
    "keypoints_client = debater_api.get_keypoints_client()\n",
    "domain = 'kp_based_survey_example'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kpa(sentences):\n",
    "    sentences_texts = [sentence['texts'] for sentence in sentences]\n",
    "    sentences_ids = [sentence['ids'] for sentence in sentences]\n",
    "\n",
    "    keypoints_client.upload_comments(domain=domain,\n",
    "                                     comments_ids=sentences_ids,\n",
    "                                     comments_texts=sentences_texts,\n",
    "                                     dont_split=True)\n",
    "\n",
    "    keypoints_client.wait_till_all_comments_are_processed(domain)\n",
    "\n",
    "    future = keypoints_client.start_kp_analysis_job(domain=domain, comments_ids=sentences_ids,\n",
    "                                                    run_params={'n_top_kps': 20})\n",
    "\n",
    "    kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=5)\n",
    "    return kpa_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import print_results\n",
    "\n",
    "kpa_result_random_1000_2016 = run_kpa(random_sample_sentences_2016)\n",
    "print_results(kpa_result_random_1000_2016, n_sentences_per_kp=2, title='Random sample 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improve coverage by taking highest quality sentences\n",
    "bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_quality_client = debater_api.get_argument_quality_client()\n",
    "\n",
    "arg_quality_scores = arg_quality_client.run(\n",
    "            [{'sentence': sentence['texts'], 'topic': 'Austin'} for sentence in sentences_2016])\n",
    "sorted_aq_sentences_2016 = [sentence for _, sentence in sorted(zip(arg_quality_scores, sentences_2016), key=lambda x: x[0], reverse=True)]\n",
    "top_aq_sentences_2016 = sorted_aq_sentences_2016[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!!!Show top/lower AQ sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import split_sentences_to_lines\n",
    "\n",
    "top_10_sentences = sorted_aq_sentences_2016[:10]\n",
    "top_10_sentences = [sentence['texts'] for sentence in top_10_sentences]\n",
    "print('top_10_sentences: ')\n",
    "print('\\n'.join(split_sentences_to_lines(top_10_sentences, 1)))\n",
    "\n",
    "bottom_10_sentences = sorted_aq_sentences_2016[-10:]\n",
    "bottom_10_sentences = [sentence['texts'] for sentence in bottom_10_sentences]\n",
    "print('\\n\\nbottom_10_sentences: ')\n",
    "print('\\n'.join(split_sentences_to_lines(bottom_10_sentences, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kpa_result_top_aq_1000_2016 = run_kpa(top_aq_sentences_2016)\n",
    "print_results(kpa_result_top_aq_1000_2016, n_sentences_per_kp=2, title='Top aq 2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kpa(sentences):\n",
    "    sentences_texts = [sentence['texts'] for sentence in sentences]\n",
    "    sentences_ids = [sentence['ids'] for sentence in sentences]\n",
    "\n",
    "    keypoints_client.upload_comments(domain=domain,\n",
    "                                     comments_ids=sentences_ids,\n",
    "                                     comments_texts=sentences_texts,\n",
    "                                     dont_split=True)\n",
    "\n",
    "    keypoints_client.wait_till_all_comments_are_processed(domain)\n",
    "\n",
    "    future = keypoints_client.start_kp_analysis_job(domain=domain, comments_ids=sentences_ids,\n",
    "                                                    run_params={'n_top_kps': 20, \n",
    "                                                                'clustering_threshold': 0.95, \n",
    "                                                                'mapping_threshold': 0.95})\n",
    "\n",
    "    kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=5)\n",
    "    return kpa_result, future.get_job_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_top_aq_1000_2016, kpa_top_aq_1000_2016_job_id = run_kpa(top_aq_sentences_2016)\n",
    "print_results(kpa_result_top_aq_1000_2016, n_sentences_per_kp=2, title='Top aq 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "show one KP: top 5 matches bottom 5 matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import print_top_and_bottom_matches_for_kp\n",
    "\n",
    "\n",
    "print_top_and_bottom_matches_for_kp(kpa_result_top_aq_1000_2016, 'Traffic congestion needs major improvement', 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_2017 = list(filter(lambda sentence: sentence['Year'] == '2017', sentences))\n",
    "arg_quality_scores = arg_quality_client.run(\n",
    "            [{'sentence': sentence['texts'], 'topic': 'Austin'} for sentence in sentences_2017])\n",
    "sorted_aq_sentences_2017 = [sentence for _, sentence in sorted(zip(arg_quality_scores, sentences_2017), key=lambda x: x[0], reverse=True)]\n",
    "top_aq_sentences_2017 = sorted_aq_sentences_2017[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kpa(sentences, key_points_by_job_id=None):\n",
    "    sentences_texts = [sentence['texts'] for sentence in sentences]\n",
    "    sentences_ids = [sentence['ids'] for sentence in sentences]\n",
    "\n",
    "    keypoints_client.upload_comments(domain=domain,\n",
    "                                     comments_ids=sentences_ids,\n",
    "                                     comments_texts=sentences_texts,\n",
    "                                     dont_split=True)\n",
    "\n",
    "    keypoints_client.wait_till_all_comments_are_processed(domain)\n",
    "\n",
    "    future = keypoints_client.start_kp_analysis_job(domain=domain, comments_ids=sentences_ids,\n",
    "                                                    run_params={'n_top_kps': 20, \n",
    "                                                                'clustering_threshold': 0.95, \n",
    "                                                                'mapping_threshold': 0.95},\n",
    "                                                    key_points_by_job_id=key_points_by_job_id)\n",
    "\n",
    "    kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=5)\n",
    "    return kpa_result, future.get_job_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_top_aq_1000_2017, _ = run_kpa(top_aq_sentences_2017, kpa_top_aq_1000_2016_job_id)\n",
    "print_results(kpa_result_top_aq_1000_2017, n_sentences_per_kp=2, title='Top aq 2017, using 2016 key points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from austin_utils import compare_results\n",
    "\n",
    "compare_results(kpa_result_top_aq_1000_2016, '2016', kpa_result_top_aq_1000_2017, '2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "drill down into traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_to_annotation(sentences):\n",
    "    term_wikifier_client = debater_api.get_term_wikifier_client()\n",
    "    sentence_to_annotations = {}\n",
    "    annotations_list = term_wikifier_client.run(sentences)\n",
    "    for sentence, annotations in zip(sentences, annotations_list):\n",
    "        sentence_to_annotations[sentence] = set([annotation['concept']['title'] for annotation in annotations])\n",
    "    return sentence_to_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_2016_texts = [sentence['texts'] for sentence in sentences_2016]\n",
    "sentence_to_annotations = get_sentences_to_annotation(sentences_2016_texts)\n",
    "all_annotations = [annotation for sentence in sentence_to_annotations \n",
    "                   for annotation in sentence_to_annotations[sentence]]\n",
    "all_annotations = sorted(list(set(all_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept = 'traffic'\n",
    "threshold = 0.5\n",
    "term_relater_client = debater_api.get_term_relater_client()\n",
    "concept_annotation_pairs = [[concept, annotation] for annotation in all_annotations]\n",
    "scores = term_relater_client.run(concept_annotation_pairs)\n",
    "matched_annotations = [annotation for annotation, score in zip(all_annotations, scores) if score > threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Top 5 matched annotations:')\n",
    "# print('\\n'.join(split_sentences_to_lines(top_10_sentences, 1)))\n",
    "print(matched_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences_texts = [sentence for sentence in sentences_2016_texts \n",
    "                     if len(sentence_to_annotations[sentence].intersection(matched_annotations)) > 0]\n",
    "print('Running over %d sentences' % len(matched_sentences_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences = [sentence for sentence in sentences_2016 if sentence['texts'] in matched_sentences_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_traffic_2016, _ = run_kpa(matched_sentences, None)\n",
    "print_results(kpa_result_traffic_2016, n_sentences_per_kp=2, title='Traffic KPA 2016')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
