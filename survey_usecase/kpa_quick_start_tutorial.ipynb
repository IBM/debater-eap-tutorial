{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7f1683b",
   "metadata": {},
   "source": [
    "# Using *Key Point Analysis* service for analyzing and finding insights in a survey data \n",
    "\n",
    "#### **Important Notice**: This tutorial describes the legacy SDK, of debater-python-api version up to 4.3.2. The tutorial of the updated SDK, starting from debater-python-api version 5.0.0, is available [here](new_sdk/kpa_quick_start_tutorial-with_results.ipynb).\n",
    "\n",
    "When you have a large collection of texts representing peopleâ€™s opinions (such as product reviews, survey answers or  social media), it is difficult to understand the key issues that come up in the data. Going over thousands of comments is prohibitively expensive.  Existing automated approaches are often limited to identifying recurring phrases or concepts and the overall sentiment toward them, but do not provide detailed or actionable insights.\n",
    "\n",
    "In this tutorial you will gain hands-on experience in using *Key Point Analysis* (KPA) for analyzing and deriving insights from open-ended answers.  \n",
    "\n",
    "The data we will use is a [community survey conducted in the city of Austin](https://data.austintexas.gov/dataset/Community-Survey/s2py-ceb7). In this survey, the citizens of Austin were asked \"If there was ONE thing you could share with the Mayor regarding the City of Austin (any comment, suggestion, etc.), what would it be?\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab28f81",
   "metadata": {},
   "source": [
    "## 1. Run *Key Point Analysis* (data from 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85aed7e",
   "metadata": {},
   "source": [
    "Lets first import all required packages for this tutoarial and initialize the *Key Point Analysis* client. The client prints information using the logger and a suitable verbosity level should be set. The client object is configured with an API key. It should be  retrieved from the [Project Debater Early Access Program](https://early-access-program.debater.res.ibm.com/) site.  In the code bellow it is passed by the enviroment variable *DEBATER_API_KEY* (you may also modify the code and place the api-key directly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510552af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from debater_python_api.api.clients.keypoints_client import KpAnalysisClient, KpAnalysisTaskFuture\n",
    "from debater_python_api.api.clients.key_point_analysis.KpAnalysisUtils import KpAnalysisUtils\n",
    "import os\n",
    "import csv\n",
    "import random\n",
    "\n",
    "KpAnalysisUtils.init_logger()\n",
    "api_key = os.environ['DEBATER_API_KEY']\n",
    "host = 'https://keypoint-matching-backend.debater.res.ibm.com'\n",
    "keypoints_client = KpAnalysisClient(api_key, host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71725a3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 Read the data and run *key point analysis*  over it\n",
    "Let's read the data from *dataset_austin.csv* file, which holds the Austin survey dataset, and print the first comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset_austin.csv') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    comments = [dict(d) for d in reader]\n",
    "\n",
    "print(f'There are {len(comments)} comments in the dataset')\n",
    "print(comments[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b55e5e",
   "metadata": {},
   "source": [
    "Each comment is a dictionary with an unique_id 'id' and 'text' and a 'year'. We will first remove all comments with text longer than 1000 characters since this is a systme's limit. Then we will filter the comments and take the ones from 2016. \n",
    "\n",
    "The *Key Point Analysis* service is able to run over hundreds of thousands of sentences, however since the computation is heavy in resources (particularly GPUs) the trial version is limited to 1000 comments. You may request to increase this limit if needed. Since we want the tutorial to be relativly fast and lightweight, we will only run on a sample of 400 comments. Note that running over a larger set improves both the quality and coverage of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = [c for c in comments if len(c['text'])<=1000]\n",
    "comments_2016 = [c for c in comments if c['year'] == '2016']\n",
    "sample_size = 400\n",
    "random.seed(0)\n",
    "comments_2016_sample = random.sample(comments_2016, sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ebe73",
   "metadata": {},
   "source": [
    "*Key point analysis* is a novel and promising approach for summarization, with an important quantitative angle. This service summarizes a collection of comments on a given topic as a small set of key points. The salience of each key point is given by the number of its matching sentences in the given comments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4b8dc",
   "metadata": {},
   "source": [
    "In order to run *Key Point Analysis*, do the following steps:\n",
    "\n",
    "### 1.2 Create a domain\n",
    "The *Key Point Analysis* service stores the data (and cached-results) in a *domain*. A user can create several domains, one for each dataset. Domains are only accessible to the user who created them.\n",
    "\n",
    "Create a domin using the **keypoints_client.create_domain(domain=domain, domain_params={})** method. Several params can be passed when creating a domain in the domain_params dictionary as described in the documentation. Leaving it empty gives us a good default behaviour. You can also use **KpAnalysisUtils.create_domain_ignore_exists(client=keypoints_client, domain=domain, domain_params={})** if you don't want an exception to be thrown if the domain already exists (note that in such case the domain_params are not updated and are remained as they where before). In this tutorial we will first delete the domain if it exists, since we want to start with an empty domain.\n",
    "\n",
    "Full documentation of the supported *domain_params* and how they affect the domain can be found [here](kpa_parameters.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4956e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'austin_demo'\n",
    "KpAnalysisUtils.delete_domain_ignore_doesnt_exist(client=keypoints_client, domain=domain)\n",
    "keypoints_client.create_domain(domain=domain, domain_params={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd618c78",
   "metadata": {},
   "source": [
    "Few domain related points:\n",
    "* We can always delete a domain we no longer need using: **KpAnalysisUtils.delete_domain_ignore_doesnt_exist(client=keypoints_client, domain=domain)**\n",
    "* Keep in mind that a domain has a state. It stores all comments that had beed uploaded into it and a cache with all calculations performed over this data.\n",
    "* If we want to restart and run over the domain from scratch (no comments and no cache), we can delete the domain and then re-create it or obviously use a different domain. Keep in mind that the cache is also cleared and consecutive runs will take longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c51cd",
   "metadata": {},
   "source": [
    "### 1.3 Upload comments into the domain\n",
    "Upload the comments into the domain using the **keypoints_client.upload_comments(domain=domain, comments_ids=comments_ids, comments_texts=comments_texts)** method. This method receives the domain, a list of comment_ids and a list of comment_texts. When uploading comments into a domain, the *Key Point Analysis* service splits the comments into sentences and runs a minor cleansing on the sentences. If you have domain-specific knowladge and want to split the comments into sentences yourself, you can upload comments that are already splitted into sentences and set the *dont_split* parameter to True (in the domain_params when creating the domain) and *Key Point Analysis* will use the provided sentences as is. \n",
    "\n",
    "Note that:\n",
    "* Comments_ids must be unique\n",
    "* The number of comments_ids must match the number comments_texts\n",
    "* Comments_texts must not be longer than 1000 characters\n",
    "* Uploading the same comment several times (same domain + comment_id, comment_text is ignored) is not a problem and the comment is only uploaded once (if the comment_text is different, it is NOT updated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5d650",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_texts = [comment['text'] for comment in comments_2016_sample]\n",
    "comments_ids = [comment['id'] for comment in comments_2016_sample]\n",
    "keypoints_client.upload_comments(domain=domain, comments_ids=comments_ids, comments_texts=comments_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b1da7",
   "metadata": {},
   "source": [
    "### 1.4 Wait for the comments to be processed\n",
    "Comments that are uploaded to the domain are being processed. This takes some times and runs in an async manner. We can't run an analysis before this phase finishes and we need to wait till all comments in the domain are processed using the **keypoints_client.wait_till_all_comments_are_processed(domain=domain)** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220393ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_client.wait_till_all_comments_are_processed(domain=domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da852b04",
   "metadata": {},
   "source": [
    "### 1.5 Start a Key Point Analysis job\n",
    "Start a *Key Point Analysis* job using the **future = keypoints_client.start_kp_analysis_job(domain=domain, run_params=run_params)** method. This method receives the domain and a *run_params*. The run_params is a dictionary with various parameters for customizing the job. Leaving it empty gives us a good default behaviour. The job runs in an async manner therefore the method returns a future object.\n",
    "\n",
    "Few additional options when running an analysis job:\n",
    "* The analysis is performed over all comments in the domain. If we need to run over a subset of the comments (split the data by different GEOs/users types/timeframes etc') we can pass a list of comments_ids to the comments_ids parameter and it will create an analysis using only the provided comments.\n",
    "* By default, key points are extracted automatically. When we want to provide key points and match all sentences to these key points we can do so by passing them to the keypoints parameter: **run_param['keypoints'] = [...]**. This enables a mode of work named human-in-the-loop where we first automatically extract key points, then we manually edit them (refine non-perfect key points, remove duplicated and add missing ones) and then run again, this time providing the edited keypoints as a given set of key points.\n",
    "* It is also possible to provide key points and let KPA add additional missing key points. To do so pass the key points to the keypoint_candidates parameter: **run_param['keypoint_candidates'] = [...]** (see section 4 for an elaborated example).\n",
    "* Full documentation of the supported *domain_params* and *run_params* and how they affect the analysis can be found [here](kpa_parameters.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bf751",
   "metadata": {},
   "outputs": [],
   "source": [
    "future = keypoints_client.start_kp_analysis_job(domain=domain, run_params={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e4414",
   "metadata": {},
   "source": [
    "### 1.6 Wait for the Key Point Analysis job to finish\n",
    "Use the returned future and wait till results are available using the **kpa_result = future.get_result()** method. The method waits for the job to finish and eventually returns the result. The result is a dictionary containing the key points (sorted descendingly according to number of matched sentences) and for each key point has a list of matched sentences (sorted descendingly according to their match score). An additional 'none' key point is added which holds all the sentences that don't match any key point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455eee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_2016 = future.get_result(high_verbosity=True, polling_timout_secs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b3a24",
   "metadata": {},
   "source": [
    "Let's print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "KpAnalysisUtils.print_result(kpa_result_2016, n_sentences_per_kp=2, title='2016 Random sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74421ee6",
   "metadata": {
    "tags": []
   },
   "source": [
    "We can also save the results to file. This creates two files, one with the key points and all matched sentences and another summary file with only the key points and their saliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ed2e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "KpAnalysisUtils.write_result_to_csv(kpa_result_2016, 'austin_survey_2016_kpa_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a5ab0",
   "metadata": {},
   "source": [
    "It is always possible to cancel a pending/running job in the following way:\n",
    "* **keypoints_client.cancel_kp_extraction_job(\\<Job Id\\>)**\n",
    "\n",
    "Job Id can be found: \n",
    "1. It's printed when a job is started \n",
    "2. From the future object: **future.get_job_id()**\n",
    "3. From user report: **keypoints_client.get_full_report()** (see bellow)\n",
    "\n",
    "It is also possibe to stop all jobs in a domain, or even all jobs in all domains (might be simpler since there is no need of the job_id):\n",
    "* **keypoints_client.cancel_all_extraction_jobs_for_domain(domain)**\n",
    "* **keypoints_client.cancel_all_extraction_jobs_all_domains()**\n",
    "\n",
    "Please cancel long jobs if the results are no longer needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc10ed9-3a2b-44b9-9375-2e4a6a5bf003",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.7 Modify the run_params and increase coverage\n",
    "Each domain has a cache that stores all intermediate results that are calculated during the analysis. Therefore modifing the run_params and running another analysis runs much faster and all intersecting intermediate results are retreived from cache. \n",
    "\n",
    "Let's run again, but now change the **mapping_policy**. The **mapping_policy** is used when mapping all sentences to the final key points: the default value is **NORMAL**. Changing to **STRICT** will cause only the sentence and key point pairs with very high matching confidence to be considered matched, increasing precision but potentially decreasing coverage. We will change it to **LOOSE**, which matches also sentences and key points with lower confidence, and is therefore expected to increase coverage at cost of precision. We will also increase the number of required key points to 100 using the **n_top_kps** parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = {'mapping_policy':'LOOSE', 'n_top_kps': 100}\n",
    "future = keypoints_client.start_kp_analysis_job(domain=domain, run_params=run_params)\n",
    "kpa_result_2016 = future.get_result(high_verbosity=True, polling_timout_secs=30)\n",
    "KpAnalysisUtils.write_result_to_csv(kpa_result_2016, 'austin_survey_2016_kpa_results.csv')\n",
    "KpAnalysisUtils.print_result(kpa_result_2016, n_sentences_per_kp=2, title='Random sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61557e9",
   "metadata": {},
   "source": [
    "By changing the mapping policy to **LOOSE** and increasing the number of key points, the coverage was increased from 44% to 68%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523a7d0",
   "metadata": {},
   "source": [
    "### 1.8 User Report\n",
    "When we want to see what domains we have, maybe delete old ones that are not needed, see past and present analysis jobs, perhaps take their job_id and fetch their result \n",
    "(via **KpAnalysisTaskFuture(keypoints_client, \\<job_id\\>).get_result()** ), \n",
    "we can get a report with all the needed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10280d7-d91f-4d0b-b634-5d2dac0900bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = keypoints_client.get_full_report()\n",
    "KpAnalysisUtils.print_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f6d46c",
   "metadata": {},
   "source": [
    "## 2. Mapping sentences to multiple key points, and creating a Key-Points-Graphs\n",
    "By default, each sentence is mapped to one key point at most (the key point with the highest match-score, that follows the **mapping_policy**). We can run again and ask KPA to map each sentence to all key points that are matched according to the **mapping_policy**, by adding the **sentence_to_multiple_kps** parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = {'sentence_to_multiple_kps': True, 'n_top_kps': 100}\n",
    "future = keypoints_client.start_kp_analysis_job(domain=domain, run_params=run_params)\n",
    "kpa_2016_job_id = future.get_job_id() # saving the job_id for a following section\n",
    "kpa_result_2016 = future.get_result(high_verbosity=True, polling_timout_secs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "KpAnalysisUtils.print_result(kpa_result_2016, n_sentences_per_kp=2, title='Random sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d94bd",
   "metadata": {},
   "source": [
    "Now that sentences are mapped to multiple key points, it is possible to create a *key points graph* by first saving the results as before, then translating the results file into a graph-data json file, then load this json file in our demo graph visualization, available at: [key points graph demo](https://keypoint-matching-ui.ris2-debater-event.us-east.containers.appdomain.cloud/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "KpAnalysisUtils.write_result_to_csv(kpa_result_2016, 'austin_survey_2016_multiple_kpa_results.csv')\n",
    "KpAnalysisUtils.generate_graphs_and_textual_summary('austin_survey_2016_multiple_kpa_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818fd69a",
   "metadata": {},
   "source": [
    "**generate_graphs_and_textual_summary** creates 4 files:\n",
    "* **austin_survey_2016_multiple_kpa_results_graph_data.json**: a graph_data file that can be loaded to: [key points graph demo](https://keypoint-matching-ui.ris2-debater-event.us-east.containers.appdomain.cloud/). It presents the relations between the key points as a graph of key points.\n",
    "* **austin_survey_2016_multiple_kpa_results_hierarchical_graph_data.json**: another graph_data file that can be loaded to the graph-demo-site. This graph is simplified, it's more convenient to extract insights from it.\n",
    "* **austin_survey_2016_multiple_kpa_results_hierarchical.txt**: This textual file shows the simplified graph (from the previous bullet) as a list of hierarchical bullets.\n",
    "* **austin_survey_2016_multiple_kpa_results_hierarchical.docx**: This Microsoft Word document shows the textual bullets (from the previous bullet) as a user-friendly report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff92aa",
   "metadata": {},
   "source": [
    "## 3. Run *Key Point Analysis* incrementally\n",
    "### 3.1 Run *Key Point Analysis* incrementally on new data (data from 2016 + 2017)\n",
    "A year passed, and we collect additional data (data from 2017). We can now upload the 2017 data to the same domain (austin_demo) and have both 2016 and 2017 data in one domain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5f9111",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_2017 = [c for c in comments if c['year'] == '2017']\n",
    "random.seed(0)\n",
    "comments_2017_sample = random.sample(comments_2017, sample_size)\n",
    "\n",
    "domain = 'austin_demo'\n",
    "comments_texts = [comment['text'] for comment in comments_2017_sample]\n",
    "comments_ids = [comment['id'] for comment in comments_2017_sample]\n",
    "keypoints_client.upload_comments(domain=domain, comments_ids=comments_ids, comments_texts=comments_texts)\n",
    "keypoints_client.wait_till_all_comments_are_processed(domain=domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cdd33f",
   "metadata": {},
   "source": [
    "We can now run a new analysis over all the data in the domain, as we did before, and automatically extract new key points. We can assume that some will be identical to the key points extracted on the 2016 data, some will be similar and some key points will be new.\n",
    "\n",
    "A better option is to run a new analysis but provide the keypoints from the 2016 analysis and let *Key Point Analysis* add new key points of 2017 data if there are such. One benefit of this approach is that the new result will mostly use 2016 key point and we will be able to compare between them, see what changed, what improved and what not. Another major benefit for this approach is run-time. 2016 data was already analyzed with these key points and since we have a cache in place much of the computation can be avoided. The 2016 key points can be provided via the: **run_param['keypoint_candidates'] = [...]** parameter, passing a list of strings, or we can use: **run_param['keypoint_candidates_by_job_id'] = <job_id>** and provide the job_id of an analysis job. KPA will take the key points from the job's result automatically. We will use this parameter and provide the *kpa_2016_job_id* we saved in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b4a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = {'sentence_to_multiple_kps': True,\n",
    "              'keypoint_candidates_by_job_id': kpa_2016_job_id, 'n_top_kps': 100}\n",
    "future = keypoints_client.start_kp_analysis_job(domain=domain, run_params=run_params)\n",
    "kpa_result_2016_2017 = future.get_result(high_verbosity=True, polling_timout_secs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a0af0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "KpAnalysisUtils.write_result_to_csv(kpa_result_2016_2017, 'austin_survey_2016_2017_kpa_results.csv')\n",
    "KpAnalysisUtils.compare_results(kpa_result_2016, kpa_result_2016_2017, '2016', '2016 + 2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8412e2",
   "metadata": {},
   "source": [
    "### 3.2 Run *Key Point Analysis* incrementaly on new data (2017 independantly)\n",
    "Using the **comments_ids** parameter in **start_kp_analysis_job** method, we can run over a subset of the comments in the domain. Let's do that and run an analysis over 2017 comments independantly. We will provide the key points from 2016 since we want to able to compare between them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d574a490",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "comments_ids = [comment['id'] for comment in comments_2017_sample]\n",
    "run_params = {'sentence_to_multiple_kps': True,\n",
    "              'keypoint_candidates_by_job_id': kpa_2016_job_id, 'n_top_kps': 100}\n",
    "future = keypoints_client.start_kp_analysis_job(comments_ids=comments_ids, domain=domain, run_params=run_params)\n",
    "kpa_result_2017 = future.get_result(high_verbosity=True, polling_timout_secs=30)\n",
    "\n",
    "KpAnalysisUtils.write_result_to_csv(kpa_result_2017, 'austin_survey_2017_kpa_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8daaef-3aac-43a0-bae7-7b3ef323e3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "KpAnalysisUtils.compare_results(kpa_result_2016, kpa_result_2017, '2016', '2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa5b2f6",
   "metadata": {},
   "source": [
    "Running over subsets of the data in the domain enable us to compare results between them (subsets can be data from different GEOs, different organizations, different users (e.g. promoters/detractors) etc')."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de996c7b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 4. Run *Key Point Analysis* on each stance separately\n",
    "In many use-cases (surveys, customer feedback, etc') the comments have positive and/or negative stance, and it is usful to create a KPA analysis on each stance seperatly. Most stance detection models don't perfome too well on survey data (also costumer feedbacks etc') since the comments tend to have many \"suggestions\" in them, and the suggestions tend to apear positive to the model while the user suggests to improve something that needs improvement.\n",
    "For that end we trained a stance-model that handles suggestions well and labels each sentence as 'Positive', 'Negative', 'Neutral' and 'Suggestion'. We usually treat Suggestions as negatives and run two separate analysis, first over 'Positive' sentences and second over 'Negative' and 'Suggestions' sentences.\n",
    "\n",
    "This has the following advantages:\n",
    "* Creates a separate positive/negative summary that shows clearly what works well and what needs to be improved.\n",
    "* Filters-out neutral sentences that usually don't contain valuable information.\n",
    "* Helps the matching model avoid stance mistakes (matching a positive sentence to a negative key point and vice-versa).\n",
    "\n",
    "Lets run again, over the Austin survey dataset, but this time create two seperate KPA analyses (positive and negative). We will first need to create a new domain and add the domain_param **do_stance_analysis**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd950d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "domain = 'austin_demo_two_stances'\n",
    "domain_params = {'do_stance_analysis': True}\n",
    "KpAnalysisUtils.delete_domain_ignore_doesnt_exist(client=keypoints_client, domain=domain)\n",
    "keypoints_client.create_domain(domain=domain, domain_params=domain_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f04dd7e",
   "metadata": {},
   "source": [
    "Let's upload all 2016 comments to the new domain and wait for them to be processed. This time the sentences' stance is also calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbbd3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_texts = [comment['text'] for comment in comments_2016]\n",
    "comments_ids = [comment['id'] for comment in comments_2016]\n",
    "keypoints_client.upload_comments(domain=domain, comments_ids=comments_ids, comments_texts=comments_texts)\n",
    "keypoints_client.wait_till_all_comments_are_processed(domain=domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd699f",
   "metadata": {},
   "source": [
    "We can download the processed sentences and save them into a csv if we want to examine the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f97380",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = keypoints_client.get_sentences_for_domain(domain=domain)\n",
    "KpAnalysisUtils.write_sentences_to_csv(sentences, f'{domain}_sentences.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae7368",
   "metadata": {},
   "source": [
    "And now, run two analyses, one over the positive sentences and one over the negative + suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = {'sentence_to_multiple_kps': True, \"n_top_kps\":100}\n",
    "run_params['stances_to_run'] = ['pos']\n",
    "run_params['stances_threshold'] = 0.5\n",
    "future = keypoints_client.start_kp_analysis_job(domain=domain, run_params=run_params)\n",
    "kpa_pos_result = future.get_result(high_verbosity=True, polling_timout_secs=30)\n",
    "KpAnalysisUtils.print_result(kpa_pos_result, n_sentences_per_kp=2, title='Random sample positives')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e9d628",
   "metadata": {},
   "source": [
    "As in many surveys, most comments are negative/suggestions therefore the positive analysis is relativly limited. Let's see how the negative analysis goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160cbb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params['stances_to_run'] = ['neg', 'sug']\n",
    "run_params['stances_threshold'] = 0.5\n",
    "future = keypoints_client.start_kp_analysis_job(domain=domain, run_params=run_params, comments_ids=comments_ids)\n",
    "kpa_neg_result = future.get_result(high_verbosity=True, polling_timout_secs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8121d10b",
   "metadata": {},
   "source": [
    "Lets print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8015b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "KpAnalysisUtils.print_result(kpa_neg_result, n_sentences_per_kp=2, title='Random sample negatives')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1e4a64",
   "metadata": {},
   "source": [
    "Reaching a nice 67% coverage, most of the sentences are matched to the 100 automatically extracted key points.\n",
    "\n",
    "We can increase the stances_threshold when we want to run over less sentences with a stronger stance. This is useful when we have a large dataset with many less-relevant sentences and we want to filter them out.\n",
    "\n",
    "We can mark the stance in the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b27b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_pos_result = KpAnalysisUtils.set_stance_to_result(kpa_pos_result, 'pos')\n",
    "kpa_neg_result = KpAnalysisUtils.set_stance_to_result(kpa_neg_result, 'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d9976f",
   "metadata": {},
   "source": [
    "And save the results (both pos/neg seperatly and merged) and create key points graphs' data files as we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c03646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_result_file = 'austin_survey_2016_pro_kpa_results.csv'\n",
    "KpAnalysisUtils.write_result_to_csv(kpa_pos_result, pos_result_file)\n",
    "KpAnalysisUtils.generate_graphs_and_textual_summary(pos_result_file)\n",
    "\n",
    "neg_result_file = 'austin_survey_2016_neg_kpa_results.csv'\n",
    "KpAnalysisUtils.write_result_to_csv(kpa_neg_result, neg_result_file)\n",
    "KpAnalysisUtils.generate_graphs_and_textual_summary(neg_result_file)\n",
    "\n",
    "kpa_merged_result = KpAnalysisUtils.merge_two_results(kpa_pos_result, kpa_neg_result)\n",
    "merged_result_file = 'austin_survey_2016_merged_kpa_results.csv'\n",
    "KpAnalysisUtils.write_result_to_csv(kpa_merged_result, merged_result_file)\n",
    "KpAnalysisUtils.generate_graphs_and_textual_summary(merged_result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2922bee",
   "metadata": {},
   "source": [
    "We can also use the incremental approach when running on each stance seperatly. We will need to provide the job_id of the positive analysis of 2016 when running on the positive sentences of 2016 + 2017 and the job_id of negative analysis of 2016 when running on the negative sentences of 2016 + 2017, but for simplicity reasons, we didn't combine the features in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd476d4",
   "metadata": {},
   "source": [
    "## 5. Cleanup\n",
    "If you finished the tutorial and no longer need the domains and the results, cleaning up is always advised:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27faf5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "KpAnalysisUtils.delete_domain_ignore_doesnt_exist(client=keypoints_client, domain='austin_demo')\n",
    "KpAnalysisUtils.delete_domain_ignore_doesnt_exist(client=keypoints_client, domain='austin_demo_two_stances')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8dbc94e",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "In this tutorial, we showed how to use the *Key Point Analysis* service, and how it provides detailed insights over survey data right out of the box - significantly reducing the effort required by a data scientist to analyze the data. We also demonstrated key *key point analysis* features such as how to modify the analysis parameters and increase coverage, how to use the stance-model and create per-stance results, how to create *key points graph* and further improve the quality and the clarity of the results, and how to incrementally add new data.\n",
    "\n",
    "Feel free to contact us for questions or assistance: *yoavka@il.ibm.com*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}