{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4648cc91",
   "metadata": {},
   "source": [
    "# Using the Debater services for analzing and finding insights in the Austin Survey dataset \n",
    "When having a large collection of texts (product reviews, survey answers, call logs transcripts etc.) it is hard to understand the key issues that come up in the data. Going over thousands of comments is just not possible. In this tutorial you will gain an hands-on experience and use the Debater services for analyzing open-ended answers for surveys and derive insights out of them. The data we will use is a community survey conducted in the city of Austin in the years 2016 and 2017 (https://data.world/cityofaustin/mf9f-kvkk). In this survey, the citizens of Austin where asked \"If there was ONE thing you could share with the Mayor regarding the City of Austin (any comment, suggestion, etc.), what would it be?\". We will analyze their open-ended answers in different ways by using four Debater services, the *Argument Quality* service, the *Key Point Analysis (KPA)* service, the *Term Wikifier* service and the *Term Relater* service, and we will see how they can be combined into a powerful text analysis tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae879d28",
   "metadata": {},
   "source": [
    "## 1. Run *Key Point Analysis* on 1000 randomly selected sentences from 2016 survey"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67140a",
   "metadata": {},
   "source": [
    "### 1.1 Read random sample of 1000 sentences from 2016 comments\n",
    "Lets take a look at the first 5 lines in the dataset_austin_sentences.csv file attached, holding the Austin survey dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('./dataset_austin_sentences.csv', 'r')\n",
    "lines = file.readlines()\n",
    "print('\\n'.join(lines[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cc843e",
   "metadata": {},
   "source": [
    "The file has all the survey answers after being split into sentences. One sentence per row. Each sentence have the following attributes: \\['id', 'text', 'district','year'\\]. We will first read the attached csv file into the 'sentences' parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d0866e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "\n",
    "with open('./dataset_austin_sentences.csv') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    sentences = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92915bc",
   "metadata": {},
   "source": [
    "Lets have a look at the sentences at hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09955a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %d sentences in the dataset' % len(sentences))\n",
    "print('Each sentence is a dictionary with the following keys: %s' % str(sentences[0].keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32f8f9c",
   "metadata": {},
   "source": [
    "Lets select only the sentences from the 2016 survey and randomly sample 1000 out of them. The *Key Point Analysis* service is able to run over hundreds of thousands of sentences, however since the computation is heavy in resources (particularly GPUs) the trial version is limited to 1000 sentences. Using a random.seed(0) is important since we already prepared a hot-cache over these sentences for a quicker *Key Point Analysis* run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0539bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_2016 = [sentence for sentence in sentences if sentence['year'] == '2016']\n",
    "print('There are %d sentences in the 2016 survey' % len(sentences_2016))\n",
    "random.seed(0)\n",
    "random_sample_sentences_2016 = random.sample(sentences_2016, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a70449d",
   "metadata": {},
   "source": [
    "### 1.2 Run *Key Point Analysis* on the random sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a48772f",
   "metadata": {},
   "source": [
    "Full documentation of the *Key Point Analysis* service can be found [here](https://early-access-program.debater.res.ibm.com/docs/services/keypoints/keypoints_pydoc.html).<br/>\n",
    "Key point analysis is a novel and promising approach for summarization, with an important quantitative angle. This service summarizes a collection of comments on a given topic as a small set of key points. The salience of each key point is given by the number of its matching sentences in the given comments.<br/>\n",
    "Before running the *Key Point Analysis* service we first need to initialize few needed parameters. The DebaterApi object supplies the clients for the various debater services. The clients print information using the logger and a suitable verbosity level is needed to be set. The api_key should be set, it can be retrieved from the early-access-program site and passed by the enviroment variable *DEBATER_API_KEY*.  The *Key Point Analysis* service stores the data (and cache) in a domain. A user can create several domains, one for each dataset. We will run all *Key Point Analysis* jobs in the same domain named 'austin_demo'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aea7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from debater_python_api.api.debater_api import DebaterApi\n",
    "from austin_utils import init_logger\n",
    "import os\n",
    "\n",
    "init_logger()\n",
    "api_key = os.environ['DEBATER_API_KEY']\n",
    "debater_api = DebaterApi(apikey=api_key)\n",
    "keypoints_client = debater_api.get_keypoints_client()\n",
    "domain = 'austin_demo'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e43af3b",
   "metadata": {},
   "source": [
    "Exercise 1:<br/>\n",
    "Lets define a method named *run_kpa*. The method receives a list of sentences (each sentence is a dictionary with the following keys: 'id','text') and runs *Key Point Analysis* on these sentences. In order to run *Key Point Analysis*, we need to:<br/>1. Upload the comments into a domain using the **keypoints_client.upload_comments(domain, comment_ids, comment_texts, dont_split=True)** method. This method receives the domain, a list of comment_ids and a list of comment_texts. When uploading comments into a domain, the *Key Point Analysis* service splits the comments into sentences and runs a minor cleansing on the sentences. Since we already splitted the comments into sentences ourselves and we want to *Key Point Analysis* service to use them as is, we will also set the *dont_split* parameter to True.<br/>2. Wait till all comments in the domain are processed using the **keypoints_client.wait_till_all_comments_are_processed(domain)** method.<br/>3. Start a *Key Point Analysis* job using the **keypoints_client.start_kp_analysis_job(domain, comments_ids, run_params)** method. This method receives the domain, a list of comment_ids and a *run_params*. The run_params is a dictionary with various parameters for customizing the job. One of the parameters we can set is *n_top_kps* which tells the system how many key points are required. We will set it to 20, therefore we will use *run_params={'n_top_kps': 20}*. The job runs in an async manner therefore the method returns a future object.<br/>4. Use the returned future and wait till results are available using the **future.get_result(high_verbosity=True)** method. The method waits for the job to finish and eventually returns the result. This result is a json with the key points (sorted descendingly according to number of matched sentences) and each key point has a list of matched sentences (sorted descendingly according to their match score). Additional 'none' key point is added which holds all the sentences that don't match any key point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fcc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kpa(sentences):\n",
    "    sentences_texts = [sentence['text'] for sentence in sentences]\n",
    "    sentences_ids = [sentence['id'] for sentence in sentences]\n",
    "\n",
    "    keypoints_client.upload_comments(domain=domain,\n",
    "                                     comments_ids=sentences_ids,\n",
    "                                     comments_texts=sentences_texts,\n",
    "                                     dont_split=True)\n",
    "\n",
    "    keypoints_client.wait_till_all_comments_are_processed(domain)\n",
    "\n",
    "    future = keypoints_client.start_kp_analysis_job(domain=domain, comments_ids=sentences_ids,\n",
    "                                                    run_params={'n_top_kps': 20})\n",
    "\n",
    "    kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=5)\n",
    "    \n",
    "    return kpa_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7baecef",
   "metadata": {},
   "source": [
    "We will now use the method you implemented and run over the random sample and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28ad8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import print_results\n",
    "\n",
    "kpa_result_random_1000_2016 = run_kpa(random_sample_sentences_2016)\n",
    "print_results(kpa_result_random_1000_2016, n_sentences_per_kp=2, title='Random sample 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0ef6cd",
   "metadata": {},
   "source": [
    "## 2. Run *Key Point Analysis* on 1000 top quality sentences from 2016 survey\n",
    "### 2.1 Select top 1000 sentences from 2016 data using the *Argument Quality* service\n",
    "The Austin Survey dataset is noisy and the answers and sentences vary in quality. Selecting the sentences randomly may lead to running over many sentences that are not informative. Running on the randomly selected sentences reached a 28.36% coverage. This means that only 28.36% of the sentences matched a key point. In order to improve the coverage and the quality of our results, we will now run over higher quality sentences and select the 1000 sentences with the highest *Argument Quality* score. The *Argument Quality* service receives pairs of \\[sentence, topic\\] and returns a score indicating whether the sentence is phrased in acceptable, clean and clear English, and whether the sentence, as is, fits to be included in a speech supporting the pairing topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c64a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import print_top_and_bottom_k_sentences\n",
    "\n",
    "def get_top_quality_sentences(sentences, top_k, topic):    \n",
    "    arg_quality_client = debater_api.get_argument_quality_client()\n",
    "    sentences_topic = [{'sentence': sentence['text'], 'topic': topic} for sentence in sentences]\n",
    "    arg_quality_scores = arg_quality_client.run(sentences_topic)\n",
    "    sentences_and_scores = zip(sentences, arg_quality_scores)\n",
    "    sentences_and_scores_sorted = sorted(sentences_and_scores, key=lambda x: x[1], reverse=True)\n",
    "    sentences_sorted = [sentence for sentence, _ in sentences_and_scores_sorted]\n",
    "    print_top_and_bottom_k_sentences(sentences_sorted, 10)\n",
    "    return sentences_sorted[:1000]\n",
    "\n",
    "sentences_2016_top_1000_aq = get_top_quality_sentences(sentences_2016, 1000, 'Austin is a great place to live')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d235a",
   "metadata": {},
   "source": [
    "### 2.2 Run *Key Point Analysis* over the selected sentences\n",
    "We will now run the *run_kpa* method over the top 1000 quality sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3574ee6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_top_aq_1000_2016 = run_kpa(sentences_2016_top_1000_aq)\n",
    "print_results(kpa_result_top_aq_1000_2016, n_sentences_per_kp=2, title='Top aq 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7064c9c",
   "metadata": {},
   "source": [
    "### 2.3 Increase coverage by decreasing the matching threshold\n",
    "Exercise 2:<br/>\n",
    "Running over higher quality sentences we managed to increase our coverage to 41.05%. In order to increase the coverage a little more, we will add another parameter to the run_param called *mapping_threshold*. We will reimplement the *run_kpa* method (please copy paste the previous one and modify it) but this time the method will also receive a *threshold* parameter and we will use it in the *run_param* in the following way: **run_param={'n_top_kps': 20, 'mapping_threshold': threshold}**<br/>The mapping_threshold is responsible of deciding whether a sentences matches (supports) a key point. Therefore reducing the threshold from the 0.99 default value makes more sentences match key points and increases the coverage, at the risk of reducing the precision.<br/>In addition, the method will now return two values. The result and the job_id stored in the future (using the future.get_job_id() method). We will need this job_id in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea5a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kpa(sentences, threshold):\n",
    "    sentences_texts = [sentence['text'] for sentence in sentences]\n",
    "    sentences_ids = [sentence['id'] for sentence in sentences]\n",
    "\n",
    "    keypoints_client.upload_comments(domain=domain,\n",
    "                                     comments_ids=sentences_ids,\n",
    "                                     comments_texts=sentences_texts,\n",
    "                                     dont_split=True)\n",
    "\n",
    "    keypoints_client.wait_till_all_comments_are_processed(domain)\n",
    "\n",
    "    future = keypoints_client.start_kp_analysis_job(domain=domain, comments_ids=sentences_ids,\n",
    "                                                    run_params={'n_top_kps': 20, \n",
    "                                                                'mapping_threshold': threshold})\n",
    "\n",
    "    kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=5)\n",
    "    \n",
    "    return kpa_result, future.get_job_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77fd51d",
   "metadata": {},
   "source": [
    "Lets now run again over the top 1000 quality sentences, this time with a 0.95 threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3e83f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_top_aq_1000_2016, kpa_top_aq_1000_2016_job_id = run_kpa(sentences_2016_top_1000_aq, 0.95)\n",
    "print_results(kpa_result_top_aq_1000_2016, n_sentences_per_kp=2, title='Top aq 2016')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2977aa3",
   "metadata": {},
   "source": [
    "The coverage was indeed increased to 50.11%. Lets examine the top 5 and bottom 5 sentences that where matched to the first key point and make sure that the precision is still high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e125d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import print_top_and_bottom_matches_for_kp\n",
    "\n",
    "\n",
    "print_top_and_bottom_matches_for_kp(kpa_result_top_aq_1000_2016, 'Traffic congestion needs major improvement', 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13efb87c",
   "metadata": {},
   "source": [
    "## 3. Run *Key Point Analysis* over 2017 survey using the key points from 2016 survey\n",
    "### 3.1 Select top 1000 sentences from 2017 data using the *Argument Quality* service\n",
    "It is very useful to be able to compare between different subsets of the data (compare between different years, different districts, etc'). We will demonstrate how easy it is to compare the 2017 data to the 2016 data. A similar comparisson can be done between districts or other subsets. <br/>Lets first filter the 2017 sentences and take the top 1000 quality sentences, as done for the 2016 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ac0a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_2017 = [sentence for sentence in sentences if sentence['year'] == '2017']\n",
    "sentences_2017_top_1000_aq = get_top_quality_sentences(sentences_2017, 1000, 'Austin is a great place to live')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218f3184",
   "metadata": {},
   "source": [
    "### 3.2 Run *Key Point Analysis* over top 1000 quality 2017 sentences using the key points from 2016\n",
    "Exercise 3:<br/>\n",
    "In order to compare the 2017 sentences to 2016 sentences we will want to map the 2017 sentences to the same key points extracted on the 2016 sentences (therwise different key points could be automattically extracted on the 2017 sentences and it would be hard to compare between them).\n",
    "For this end we will reimplement the *run_kpa* method (please copy paste the previous one and modify it). This time it will receive a new *key_points_by_job_id* parameter. This parameter is passed to the *key_points_by_job_id* parameter in the **keypoints_client.start_kp_analysis_job(domain, comments_ids,run_params, key_points_by_job_id)** method. When *None* is passed to *key_points_by_job_id*, key points are automatically extracted, however when it is set with a *job_id* of a previous job it uses the key points from that job and matches all sentences to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00586ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kpa(sentences, threshold, key_points_by_job_id=None):\n",
    "    sentences_texts = [sentence['text'] for sentence in sentences]\n",
    "    sentences_ids = [sentence['id'] for sentence in sentences]\n",
    "\n",
    "    keypoints_client.upload_comments(domain=domain,\n",
    "                                     comments_ids=sentences_ids,\n",
    "                                     comments_texts=sentences_texts,\n",
    "                                     dont_split=True)\n",
    "\n",
    "    keypoints_client.wait_till_all_comments_are_processed(domain)\n",
    "\n",
    "    future = keypoints_client.start_kp_analysis_job(domain=domain, comments_ids=sentences_ids,\n",
    "                                                    run_params={'n_top_kps': 20, \n",
    "                                                                'mapping_threshold': threshold},\n",
    "                                                    key_points_by_job_id=key_points_by_job_id)\n",
    "\n",
    "    kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=5)\n",
    "    \n",
    "    return kpa_result, future.get_job_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7597c1",
   "metadata": {},
   "source": [
    "Lets use the new *run_kpa* and provide it with the *top 1000 quality sentences from 2017* and the job_id of *top 1000 quality sentences from 2016*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366c159",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_top_aq_1000_2017, _ = run_kpa(sentences_2017_top_1000_aq, 0.95, kpa_top_aq_1000_2016_job_id)\n",
    "print_results(kpa_result_top_aq_1000_2017, n_sentences_per_kp=2, title='Top aq 2017, using 2016 key points')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0eab7e",
   "metadata": {},
   "source": [
    "Since both jobs have the same key points, we can now easily compare the two results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689f390d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from austin_utils import compare_results\n",
    "\n",
    "compare_results(kpa_result_top_aq_1000_2016, '2016', kpa_result_top_aq_1000_2017, '2017')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca9696f",
   "metadata": {},
   "source": [
    "## 4. Deep dive into the *traffic problem* in Austin using the *Term Wikifier* and *Term Relater* services\n",
    "As we've seen in the 2016 results, that the traffic problem in Austin is significant. In this section we will use the *Term Wikifier* and *Term Relater* services to select a subset of the sentences related to the *traffic* topic and run *Key Point Analysis* over them. The *Term Wikifier* service runs over sentences and identifies the Wikipedia articles that are referenced by phrases or words or ideas, related to as mentions, in the sentences. The *Term Relater* service runs over pairs of terms, each being a Wikipedia title, i.e., each being a concept, this service scores how closely these terms are related, namely -- the extent of relatedness between them.<br/>\n",
    "We will use the *Term Wikifier* to extract all mentions in all sentences, then use the *Term Relater* to select a subset of these mentions which are related to the 'traffic' concept, then select all sentences that have mentions related to the 'traffic' concept and run *Key Point Analysis* over them. Running over these sentences will create key points specifically to the traffic problem in Austin and expose insights and suggestions related to it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaecd7d",
   "metadata": {},
   "source": [
    "### 4.1 Calculate the mentions in the sentences using the *Term Wikifier*\n",
    "Exercise 4:<br/>\n",
    "Please complete the missing parts in the *get_sentence_to_mentions(sentences_texts)* method. The method uses the *Term Wikifier* service, calculates the mentions for each sentence and stores it in a dictionary named *sentence_to_mentions*. The *Term Wikifier* client runs over the sentences_texts using the **term_wikifier_client.run(sentences_texts)** method and returns a list of mentions_lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d8e228",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_to_mentions(sentences_texts):\n",
    "    term_wikifier_client = debater_api.get_term_wikifier_client()\n",
    "\n",
    "    mentions_list = term_wikifier_client.run(sentences_texts)\n",
    "    \n",
    "    sentence_to_mentions = {}\n",
    "    for sentence_text, mentions in zip(sentences_texts, mentions_list):\n",
    "        sentence_to_mentions[sentence_text] = set([mention['concept']['title'] for mention in mentions])\n",
    "    return sentence_to_mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd866f46",
   "metadata": {},
   "source": [
    "Lets calculate the mentions on all 2016 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ed140",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_2016_texts = [sentence['text'] for sentence in sentences_2016]\n",
    "sentence_to_mentions = get_sentence_to_mentions(sentences_2016_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84263fd9",
   "metadata": {},
   "source": [
    "### 4.2 Find the mentions that relate to the *traffic* concept using the *Term Relater* service\n",
    "Since we're inrested in the *traffic* concept, we will now take all mentions and find the ones that are related to that concept. Then we will select all sentences that have at least one mention that is related to the *traffic* concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a2cf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_mentions = set([mention for sentence in sentence_to_mentions \n",
    "                   for mention in sentence_to_mentions[sentence]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae065c6",
   "metadata": {},
   "source": [
    "Exercise 5:<br/>\n",
    "Please complete the missing parts in the *get_related_mentions(concept, threshold, all_mentions)* method. It receives a given concept, a threshold and all_mentions. It then uses the *Term Relater* service to calculate the relatedness between the mentions and the concept and returns all mentions that have relatedness score above the given threhold. The *term_relater_client* runs over the pairs using the **term_relater_client.run(concept_mention_pairs)** method and returns a list of scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45474c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_mentions(concept, threshold, all_mentions):\n",
    "    term_relater_client = debater_api.get_term_relater_client()\n",
    "    concept_mention_pairs = [[concept, mention] for mention in all_mentions]\n",
    "\n",
    "    scores = term_relater_client.run(concept_mention_pairs)\n",
    "    \n",
    "    return [mention for mention, score in zip(all_mentions, scores) if score > threshold]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a31bfb",
   "metadata": {},
   "source": [
    "We will now use the method you've implemented and find the mentions that match the *traffic* concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748d167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_mentions = get_related_mentions('traffic', 0.5, all_mentions)\n",
    "print(matched_mentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b7ee99",
   "metadata": {},
   "source": [
    "### 4.3 Run *Key Point Analysis* over the sentences that relate to the *traffic* concept\n",
    "Let's select the sentences that have mentions that are related to the *traffic* concept and run over them. We will need to switch back from sentences_texts to sentences dictionaries since our *run_kpa* method needs the sentences dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences_texts = [sentence for sentence in sentences_2016_texts \n",
    "                     if len(sentence_to_mentions[sentence].intersection(matched_mentions)) > 0]\n",
    "matched_sentences = [sentence for sentence in sentences_2016 if sentence['text'] in matched_sentences_texts]\n",
    "print('Running over %d sentences' % len(matched_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe377e6",
   "metadata": {},
   "source": [
    "Finally, lets run over these sentences and examine the *traffic* related key points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3345c866",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result_traffic_2016, _ = run_kpa(matched_sentences, 0.95, None)\n",
    "print_results(kpa_result_traffic_2016, n_sentences_per_kp=2, title='Traffic KPA 2016')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "think_2021_debater_demo",
   "language": "python",
   "name": "think_2021_debater_demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
