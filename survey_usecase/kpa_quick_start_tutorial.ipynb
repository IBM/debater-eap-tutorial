{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7f1683b",
   "metadata": {},
   "source": [
    "# Using *Key Point Analysis* service for analyzing and finding insights in a survey data \n",
    "When you have a large collection of texts representing peopleâ€™s opinions (such as product reviews, survey answers or  social media), it is difficult to understand the key issues that come up in the data. Going over thousands of comments is prohibitively expensive.  Existing automated approaches are often limited to identifying recurring phrases or concepts and the overall sentiment toward them, but do not provide detailed or actionable insights.\n",
    "\n",
    "In this tutorial you will gain hands-on experience in using *Key Point Analysis* (KPA) for analyzing and deriving insights from open-ended answers.  \n",
    "\n",
    "The data we will use is a community survey conducted in the city of Austin (https://data.world/cityofaustin/mf9f-kvkk). In this survey, the citizens of Austin where asked \"If there was ONE thing you could share with the Mayor regarding the City of Austin (any comment, suggestion, etc.), what would it be?\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab28f81",
   "metadata": {},
   "source": [
    "## 1. Run *Key Point Analysis*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71725a3a",
   "metadata": {},
   "source": [
    "### 1.1 Read the data and run *key point analysis*  over it\n",
    "Let's read the data from *dataset_austin.csv* file, which holds the Austin survey dataset, and print the first 5 comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c6f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "\n",
    "\n",
    "with open('./dataset_austin.csv') as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    reader = [dict(d) for d in reader]\n",
    "    comments = list(reader)\n",
    "\n",
    "print(f'There are {len(comments)} comments in the dataset')\n",
    "print(comments[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b55e5e",
   "metadata": {},
   "source": [
    "Each comment is a dictionary with an unique_id 'id' and 'comment_text'. Let's randomly sample 1000 comments. The *Key Point Analysis* service is able to run over hundreds of thousands of sentences, however since the computation is heavy in resources (particularly GPUs) the trial version is limited to 1000 comments. You may request to increase this limit if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb6777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "random_sample_comments = random.sample(comments, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ebe73",
   "metadata": {},
   "source": [
    "*Key point analysis* is a novel and promising approach for summarization, with an important quantitative angle. This service summarizes a collection of comments on a given topic as a small set of key points. The salience of each key point is given by the number of its matching sentences in the given comments.\n",
    "\n",
    "Before running the *Key Point Analysis* service we first need to initialize our client. The clients print information using the logger and a suitable verbosity level should be set. The client object is configured with an API key. It should be  retrieved from the [Project Debater Early Access Program](https://early-access-program.debater.res.ibm.com/) site.  In the code bellow it is passed by the enviroment variable *DEBATER_API_KEY*.\n",
    "\n",
    "The *Key Point Analysis* service stores the data (and cached-results) in a *domain*. A user can create several domains, one for each dataset. Domains are only accessible to the user who created them.\n",
    "\n",
    "Full documentation of the *Key Point Analysis* service can be found [here](https://early-access-program.debater.res.ibm.com/docs/services/keypoints/keypoints_pydoc.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4582d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from debater_python_api.api.clients.keypoints_client import KpAnalysisClient, KpAnalysisUtils, KpAnalysisTaskFuture\n",
    "import os\n",
    "\n",
    "KpAnalysisUtils.init_logger()\n",
    "api_key = os.environ['DEBATER_API_KEY']\n",
    "host = 'https://keypoint-matching-backend.debater.res.ibm.com'\n",
    "keypoints_client = KpAnalysisClient(api_key, host)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4b8dc",
   "metadata": {},
   "source": [
    "In order to run *Key Point Analysis*, do the following steps:\n",
    "\n",
    "1. Create a domin using the **keypoints_client.create_domain(domain=domain, domain_params={})** method. Several params can be passed when creating a domain in the domain_params dictionary as described in the documentation. Leaving it empty gives us a good default behaviour. You can also use **KpAnalysisUtils.create_domain_ignore_exists(client=keypoints_client, domain=domain, domain_params={})** if you don't want an exception to be thrown if the domain already exists. Note that in such case the domain_params are not updated and are remained as they where before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4956e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'austin_demo'\n",
    "KpAnalysisUtils.create_domain_ignore_exists(client=keypoints_client, domain=domain, domain_params={})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c51cd",
   "metadata": {},
   "source": [
    "2. Upload the comments into the domain using the **keypoints_client.upload_comments(domain=domain, comments_ids=comments_ids, comments_texts=comments_texts)** method. This method receives the domain, a list of comment_ids and a list of comment_texts. When uploading comments into a domain, the *Key Point Analysis* service splits the comments into sentences and runs a minor cleansing on the sentences. If you have domain-specific knowladge and want to split the comments into sentences yourself, you can upload comments that are already splitted into sentences and set the *dont_split* parameter to True (in the domain_params when creating the domain) and *Key Point Analysis* will use the provided sentences as is. \n",
    "\n",
    "Note that:\n",
    "* Comments_ids must be unique\n",
    "* The number of comments_ids must match the number comments_texts\n",
    "* Comments_texts must not be longer than 1000 characters\n",
    "* Uploading the same comment several times (same domain + comment_id, comment_text is ignored) is not a problem and the comment is only uploaded once (if the comment_text is different, it is NOT updated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da5d650",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "comments = [c for c in random_sample_comments if len(c['comment_text'])<=1000]\n",
    "comments_texts = [comment['comment_text'] for comment in comments]\n",
    "comments_ids = [comment['id'] for comment in comments]\n",
    "keypoints_client.upload_comments(domain=domain, comments_ids=comments_ids, comments_texts=comments_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183b1da7",
   "metadata": {},
   "source": [
    "3. Comments that are uploaded to the domain are being processed. This takes some times and runs in an async manner. We can't run an analysis before this phase finishes and we need to wait till all comments in the domain are processed using the **keypoints_client.wait_till_all_comments_are_processed(domain=domain)** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220393ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_client.wait_till_all_comments_are_processed(domain=domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da852b04",
   "metadata": {},
   "source": [
    "4. Start a *Key Point Analysis* job using the **future = keypoints_client.start_kp_analysis_job(domain=domain, run_params=run_params)** method. This method receives the domain and a *run_params*. The run_params is a dictionary with various parameters for customizing the job. Leaving it empty gives us a good default behaviour. The job runs in an async manner therefore the method returns a future object.\n",
    "\n",
    "Few additional options when running an analysis job:\n",
    "* The analysis is performed over all comments in the domain. If we need to run over a subset of the comments (split the data by different GEOs/users types/timeframes etc') we can pass a list of comments_ids to the comments_ids parameter and it will create an analysis using only the provided comments.\n",
    "* By default, key points are extracted automatically. When we want to provide key points and match all sentences to these key points we can do so by passing them to the keypoints parameter: **run_param['keypoints'] = [...]**\n",
    "* It is also possible to provide key points and let KPA add additional missing key points. To do so pass the key points to the keypoint_candidates parameter: **run_param['keypoint_candidates'] = [...]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948bf751",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = {'n_top_kps': 20}  # limit number of key points to 20 for faster results\n",
    "future = keypoints_client.start_kp_analysis_job(domain=domain, run_params=run_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a1e4414",
   "metadata": {},
   "source": [
    "5. Use the returned future and wait till results are available using the **kpa_result = future.get_result()** method. The method waits for the job to finish and eventually returns the result. The result is a dictionary containing the key points (sorted descendingly according to number of matched sentences) and for each key point has a list of matched sentences (sorted descendingly according to their match score). An additional 'none' key point is added which holds all the sentences that don't match any key point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455eee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b3a24",
   "metadata": {},
   "source": [
    "Let's print the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6af7396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from austin_utils import print_results\n",
    "print_results(kpa_result, n_sentences_per_kp=2, title='Random sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74421ee6",
   "metadata": {},
   "source": [
    "We can also save the results to file. This creates two files, one with the key points and all matched sentences and another summary file with only the key points and their saliance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5ed2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "KpAnalysisUtils.write_result_to_csv(kpa_result, 'austin_survey_kpa_results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a5ab0",
   "metadata": {},
   "source": [
    "It is always possible to cancel a pending/running job in the following way:\n",
    "* **keypoints_client.cancel_kp_extraction_job(\\<Job Id\\>)**\n",
    "\n",
    "Job Id can be found: \n",
    "1. It's printed when a job is started \n",
    "2. From the fututre object: **future.get_job_id()**\n",
    "3. From user report: **keypoints_client.get_full_report()** (see bellow)\n",
    "\n",
    "It is also possibe to stop all jobs in a domain, or even all jobs in all domains:\n",
    "* **keypoints_client.cancel_all_extraction_jobs_for_domain(domain)**\n",
    "* **keypoints_client.cancel_all_extraction_jobs_all_domains()**\n",
    "\n",
    "Please cancel long jobs if the results are no longer needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1adffd",
   "metadata": {},
   "source": [
    "### 1.3 Modify the run_params and increase coverage\n",
    "Each domain have a cache that stores all intemidiate results that are canculate during the analysis. Therefore modifing the run_params and running another analysis runs much faster and all intersecting inetmidiate results are retreived from cache. \n",
    "\n",
    "Let's run again, but now reduce the **clustering_threshold** and **mapping_threshold**. The **clustering_threshold** is used for the key points selection (choose higher values for more fine-grained key points, and lower for more distinct key points). The **mapping_threshold** is used when mapping all sentences to the final key points (a lower threshold leads to a higher coverage with the risk of a lower precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199d24fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = {'n_top_kps': 20, 'clustering_threshold': 0.95, 'mapping_threshold': 0.95}  # limit number of key points to 20 for faster results\n",
    "future = keypoints_client.start_kp_analysis_job(domain=domain, run_params=run_params)\n",
    "kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03300267",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(kpa_result, n_sentences_per_kp=2, title='Random sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61557e9",
   "metadata": {},
   "source": [
    "By reducing the thresholds, the coverage was increased from 59% to 66%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f6d46c",
   "metadata": {},
   "source": [
    "## 2. Mapping setences to multiple key points, and creating Key-Points-Graphs\n",
    "By default, each sentence is mapped to one key point at most (the key point with the highest match-score, above the **mapping_threshold**). We can run again and ask KPA to map each sentence to all key points with a match-score above the **mapping_threshold**, by adding the **sentence_to_multiple_kps** parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params = {'n_top_kps': 20, 'clustering_threshold': 0.95, 'mapping_threshold': 0.95, 'sentence_to_multiple_kps': True}\n",
    "future = keypoints_client.start_kp_analysis_job(domain=domain, run_params=run_params)\n",
    "kpa_result = future.get_result(high_verbosity=True, polling_timout_secs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a89b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_results(kpa_result, n_sentences_per_kp=2, title='Random sample')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5d94bd",
   "metadata": {},
   "source": [
    "Now that sentences are mapped to multiple key points, it is possible to create a *key points graph* by first saving the results as before, then translating the results file into a graph-data json file, then load this json file in our demo graph visualization, available at: [key points graph demo](https://keypoint-matching-ui.ris2-debater-event.us-east.containers.appdomain.cloud/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9c2614",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file = 'austin_survey_kpa_results.csv'\n",
    "KpAnalysisUtils.write_result_to_csv(kpa_result, result_file)\n",
    "KpAnalysisUtils.create_graph_data_file_for_ui(result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818fd69a",
   "metadata": {},
   "source": [
    "You can now go to: [key points graph demo](https://keypoint-matching-ui.ris2-debater-event.us-east.containers.appdomain.cloud/) and load the graph's data file **austin_survey_kpa_results_graph_data.json** to the ui."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73996498",
   "metadata": {},
   "source": [
    "## 3. Run key point analysis on each stance seperatly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c190d180",
   "metadata": {},
   "source": [
    "In many use-cases (surveys, customer feedback, etc') the comments have positive and/or negative stance, and it is usful to create a KPA analysis on each stance seperatly. Most stance detection models don't perfome too well on survey data (also costumer feedbacks etc') since the comments tend to have many \"suggestions\" in them, and the suggestions tend to apear positive to the model while the user suggests to improve something that needs improvement.\n",
    "For that end we trained a stance-model that handles suggestions well and labels each sentence as 'Positive', 'Negative', 'Neutral' and 'Suggestion'. We usually treat Suggestions as negatives and run two separate analysis, first over 'Positive' sentences and second over 'Negative' and 'Suggestions' sentences.\n",
    "\n",
    "This has the following advantages:\n",
    "* Creates a separate positive/negative summary that shows clearly what works well and what needs to be improved.\n",
    "* Filters-out neutral sentences that usually don't contain valuable information.\n",
    "* Helps the matching model avoid stance mistakes (matching a positive sentence to a negative key point and vice-versa).\n",
    "\n",
    "Lets run again, over the Austin survey dataset, but this time create two seperate KPA analyses (positive and negative). We will first need to create a new domain but this time add the domain_param **do_stance_analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccc4fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = 'austin_demo_two_stances'\n",
    "domain_params = {'do_stance_analysis': True}\n",
    "KpAnalysisUtils.create_domain_ignore_exists(client=keypoints_client, domain=domain, domain_params=domain_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9612ccde",
   "metadata": {},
   "source": [
    "Let's upload the comments to the new domain and wait for them to be processed. This time the sentences' stance is also calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c91c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints_client.upload_comments(domain=domain, comments_ids=comments_ids, comments_texts=comments_texts)\n",
    "keypoints_client.wait_till_all_comments_are_processed(domain=domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bd699f",
   "metadata": {},
   "source": [
    "We can also download the processed sentences and save them into a csv if we want to examine the processed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f97380",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = keypoints_client.get_sentences_for_domain(domain=domain)\n",
    "KpAnalysisUtils.write_sentences_to_csv(sentences, f'{domain}_sentences.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958336c2",
   "metadata": {},
   "source": [
    "And now, run two analyses, one over the positive sentences and one over the negative + suggestions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ae98eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params['stances_to_run'] = ['pos']\n",
    "run_params['stances_threshold'] = 0.5\n",
    "future = keypoints_client.start_kp_analysis_job(domain=domain, run_params=run_params)\n",
    "kpa_pos_result = future.get_result(high_verbosity=True, polling_timout_secs=30)\n",
    "print_results(kpa_pos_result, n_sentences_per_kp=2, title='Random sample positives')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba84e99",
   "metadata": {},
   "source": [
    "As in many surveys, most comments are negative/suggestions therefore the positive analysis is relativly limited. Let's see how the negative analysis goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c0cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_params['stances_to_run'] = ['neg', 'sug']\n",
    "run_params['stances_threshold'] = 0.5\n",
    "future = keypoints_client.start_kp_analysis_job(domain=domain, run_params=run_params)\n",
    "kpa_neg_result = future.get_result(high_verbosity=True, polling_timout_secs=30)\n",
    "print_results(kpa_neg_result, n_sentences_per_kp=2, title='Random sample positives')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4383e7",
   "metadata": {},
   "source": [
    "Reaching a nice 70% coverage, most of the sentences are matched to the 20 automatically extracted key points.\n",
    "\n",
    "We can increase the **stances_threshold** when we want to run over less sentences with a stronger stance. This is useful when we have a large dataset with many less-relevant sentences and we want to filter them out.\n",
    "\n",
    "We can mark the stance in the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3255e204",
   "metadata": {},
   "outputs": [],
   "source": [
    "kpa_pos_result = KpAnalysisUtils.set_stance_to_result(kpa_pos_result, 'pos')\n",
    "kpa_neg_result = KpAnalysisUtils.set_stance_to_result(kpa_neg_result, 'neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dd27f3",
   "metadata": {},
   "source": [
    "And save the results (both pos/neg seperatly and merged) and create key points graphs' data files as we did before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e291c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_result_file = 'austin_survey_pos_kpa_results.csv'\n",
    "KpAnalysisUtils.write_result_to_csv(kpa_pos_result, pos_result_file)\n",
    "KpAnalysisUtils.create_graph_data_file_for_ui(pos_result_file)\n",
    "\n",
    "neg_result_file = 'austin_survey_neg_kpa_results.csv'\n",
    "KpAnalysisUtils.write_result_to_csv(kpa_neg_result, neg_result_file)\n",
    "KpAnalysisUtils.create_graph_data_file_for_ui(neg_result_file)\n",
    "\n",
    "kpa_merged_result = KpAnalysisUtils.merge_two_results(kpa_pos_result, kpa_neg_result)\n",
    "merged_result_file = 'austin_survey_merged_kpa_results.csv'\n",
    "KpAnalysisUtils.write_result_to_csv(kpa_merged_result, merged_result_file)\n",
    "KpAnalysisUtils.create_graph_data_file_for_ui(merged_result_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523a7d0",
   "metadata": {},
   "source": [
    "When we want to see what domains we have, maybe delete old ones that are not needed, see past and present analysis jobs, perhaps take their job_id and fetch their result \n",
    "(**KpAnalysisTaskFuture(keypoints_client, \\<job_id\\>).get_result()**), \n",
    "we can get a report with all the needed information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ff7729",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = keypoints_client.get_full_report()\n",
    "KpAnalysisUtils.print_report(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62a1ea1",
   "metadata": {},
   "source": [
    "## 4. Conclusion\n",
    "In this tutorial, we showed how *Key Point Analysis* is used, and how it provides detailed insights over survey data right out of the box - significantly reducing the effort required by a data scientist to analyze the data. We also demonstrated key *key point analysis* features such as how we can modify the analysis parameters and increase coverage, how we can use the stance-model and create per-stance results and how to create *key points graph* and further improve the quality and the clarity of the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kpa_env",
   "language": "python",
   "name": "kpa_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
